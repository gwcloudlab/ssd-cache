\section{Overview}

\subsection{Hit Ratio Curves}

Hit ratio curves as a function of cache size gives us an estimate of the percentage of cache hits for any given cache size. A common method to calculate the HRC is by calculating the \emph{Reuse Distance} for every block request. A Reuse Distance (RD) is the number of unique blocks between two consecutive accesses of a given block.

RD calculation for every block for a workload is computationally intensive. All the block requests for a given workload needs to be saved in the memory, and for every block request, the previous occurrence of that request has to be found and the number of unique elements between the two occurrences needs to be calculated. A naive implementation of Mattson's algorithm thus takes $\mathcal{O}(n\log{}n)$ space, where $n$ is the size of the trace, and $\mathcal{O}(n\log{}nm)$ runtime where $m$ is the number of unique blocks in the trace.

Recently there have been some works [Parda, Shards, CounterStack] that calculate RD in sublinear time with constant space. CounterStack algorithm uses a counter for each of the block request, that holds the counter for the number of blocks requested after the counter for a particular block has been initialized. Comparing the counter of a block with other blocks give the RD of that block. The space is exponentially reduced by the usage of Hyperloglog [ref] data structure for each of the counters and by various other techniques such as downsampling.

Shards on the other hand uses hashing to calculate the hash value of the block address and modulo the value by a user defined constant. If the resulting value is lesser than another user defined threshold, the block address is retained for calculating RD, if not, the block address is discarded. The authors of Shards claim that by just using the very minimal sample of uniformly distrubuted block addresses, they are able to calculate the RD values with very little error.

In our work, we chose to use a variation of Mattson's Algorithm instead of adopting Shards or CounterStack for two simple reasons- First, our algorithm has a very small time window within which we calculate the RD of the blocks that have been requested. This means that our sample is already very small, in the order of tens of Kilobytes. Using Shards or CounterStack on such a small sample size is highly inefficient and leads to inaccuracies, as these methods work well with a larger sample size. Second, we were aiming to avoid serious computationally intesive calculations for calculating RD, as we would have to do this several hundreds or thousands of times depending on our time window.

 Our variation of Mattson algorithm is shown in listingX. We maintain a global counter to count the number of block requests, and a hastable that stores the block adresses as the key and an array of indexes as it's values. This array will store the counter(s) at which the given block(key) was requested. For every block request, we look up our hashtable if the block address exists. If it does, we append the current counter value to the block address, else we insert the block address into the hastable with it's initial value as the counter's current value.

 If we want to calculate the reuse distance, usually at the end of a timewindow, the hastable is iterated one key at a time and the pairwise-difference between each of the values of the keys gives us the approximate reuse distance. A cdf of all the pairwise-distances gives us an approximate HRC as a function of the reuse distance. \note{Need better explanation}

 \subsection{Reuse Intensity}

\emph{Reuse Intesity} is used as a measure to capture sudden bursts in hit ratio, as opposed to RD which is used to capture the overall trend of the hit ratio for a given time window. For certain web services, such as stock market providers, capturing quick bursts in trend is vital. To this end, $$ RI = 1 - \frac{\text{\# unique blocks}}{\text{\# total blocks}} $$ captures the sudden bursts in Hit Ratio. The above equation indicates that, more the workload is random, the closer the value of $RI$ will be to 0. For instance, if there is only 9 unique blocks from a workload of 10 block requests, the value of $RI$ will be 0.1. On the other hand, if the workload has a lot of repeated requests to the same block, the value of $RI$ will be close to 1. The value of $RI$ once every 60 seconds, and can be tuned as a user parameter. The overhead incurred by this calculation is very minimal compared to the other computations in our algorithm. So, calcualting this value in the background/parallely to other computations results in zero overhead. \note{is this statement true?}

\subsection{Cache Utility model}


$$ h_p(c_p)  = HR(c_p) $$
$$ h_s(c_s)  = HR(c_s + c_p') - HR(c_p') $$

where, \\ 
$c_p$ is the cache size of pcie ssd, \\
$c_s$ is the cache size of ssd, \\
$h_s(c_s)$ is the hit rate of ssd as a function of its cache size, \\
$h_p(c_p)$ is the hit rate of pcie ssd as a function of its cache size.  \\

% $$ \alpha = \alpha_s + \alpha_p $$ % Do we ever need this equation?

$$   \alpha_s = \frac{c_p}{c_s + c_p} $$ \# larger cache will have a smaller alpha
$$   \alpha_p = 1-\alpha_s $$ \# smaller cache will have a larger alpha

where $\alpha_s$ and $\alpha_p$ represents the ``benefit" a particular application/workload can get from the $RI$ values of ssd and pcie ssd caches respectively. Since RI indicates sudden bursts of block reuses, this cannot be represented well with RD. RD usually represents a large distribution of block reuse distances, but higher RI frequent smaller reuse distances. The most benefit from a higher RI can be obtained only with a cache layer that has the smallest size, because smaller caches can give more benefit if we can fit in blocks that has lower RDs but are repeated frequently.


% Term 1 = cache utility from long term RD / hit rate estimate (less important for smaller cache) \\
% Term 2 = cache utility from short term bursts of accesses that are easy or hard to cache (more important for smaller cache)

$$ CU_p(c_p) = \frac{l_b}{l_p} \times h_p(c_p) + (\alpha_p \times RI) $$
$$ CU_s(c_s) = \frac{l_b}{l_s} \times h_s(c_s) + (\alpha_s \times RI) $$
$$ CU(C) = CU_p(c_p') + CU_s(c_s') + l_b (1 - h_p(c_p') + h_s(c_s')) $$ \note{Did not discuss about this yet.}

We run the optimazation algorithm in two parts/stages. Our solver uses simnulated Annealing algorithm. We split the hit ratio curve into two parts. The division point is the size of the FC. We find the optimal cache space for the VMs for FC. Once we solve for FC, we then solve the ``remaining" i.e. from size(FC) + 1 to size(SC + FC) as shown in figureX.
