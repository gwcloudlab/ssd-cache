% What is the problem with the current work?
%     HDD accesses are slow
%     Number of VMs in a host is every increasing
%         More contention
%         Each Vm has different workloads and different priorities

% What is our vision for the perfect world?
%     Little or no contention between VMs
%     Fine tune SLAs/ priorities -> maybe
%     As cheaply and efficiently as possible
%
% How does our system help with this vision?
%     We have different storage devices/systems that we can use such as SSD, PCIe SSD, NVM, etc.
%         Each storage has different cost and tradeoffs
%         (What do we want to use and why?)
%             - We are using the same cache space. So the hit ratio is going to be the same. What we get advantage of is the overall cache latency/utility (l1*h1 + l2*h2 + l3*(1-h1+h2))
%     We use a multi tier cache that spans several of those devices
%    Our Contributions:
%         Partition the cache to different VMs according to their workload and prority
%         Calculate HRCs in realtime efficiently using a variation of Mattson's algorithm
%         Use a multi constraint optimization algorithm that uses this HRC to allocate resources
%
% For a single cache layer, if the cache is very large, would addressing be a problem?

\section{Introduction}
As the hardware keeps increasing in ``power" the ratio of the number of VMs per host also keeps increasing. A typical host in a datacenter now packs tens to hundreds of VMs on a single host, in order to maximize the utilization of the resources in a given host. To this end, hosts are equiped with terabytes of hard disk space and zetta bytes of external attached storage systems.

As a result, we have managed increasing the storage capacity but the I/O latency and throughput to and from these devices haven't increased. This is due to the fact of the hardware limitation of the magnetic devices that are being attached as storage devices. To overcome this limitation, several of the datacenters are using SSD's as a caching device to increase the I/O accesses of the overall system.

The problem with these systems are that these are throughput oriented and does not take into consideration the access pattern of the VMs. As a result a ``bad" VM with huge number of random I/O that might not benefit from the cache in anyway might end up taking all the ccache space, despite there being other interactive VMs that can benefit the most from the cache end up with very less or in some extreme cases no cache space at all. On top of that, VM's can have different priorities and we have to take that into consideration as well.

In a perfect world there should me no contention among the VMs in a particular host, and we should be able to guarentee fine tuned I/O performance for each of the VMs on a host.

In this work we present a simulated multi-tier flash cache based solution that uses multiple flash caches and partitions them dynamically in runtime based on the VMs' workload and priority.

Our contributions are:
\begin{itemize}
\item Present a cache partitioning algorithm that partitions multiple caches simultaneously at runtime based on individual VMs' workload and priority.
\item Calculate Hit Ratio Curves in runtime using a variation of Mattson's algorithm. Our variation runs faster and uses very low memory. (We need to quantify all these)
\item We use a multi constraint optimization algorithm that uses the HRC to allocate resources among different caching devices.
\item We present a simulation using two different cache devices of varying throughput and latency. Using these caches with our algorithm we show that we could attain the maximum hit rate in most cases.
\end{itemize}
