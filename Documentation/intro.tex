% What is the problem with the current work?
%     HDD accesses are slow
%     Number of VMs in a host is every increasing
%         More contention
%         Each Vm has different workloads and different priorities

% What is our vision for the perfect world?
%     Little or no contention between VMs
%     Fine tune SLAs/ priorities -> maybe
%     As cheaply and efficiently as possible
%
% How does our system help with this vision?
%     We have different storage devices/systems that we can use such as SSD, PCIe SSD, NVM, etc.
%         Each storage has different cost and tradeoffs
%         (What do we want to use and why?)
%             - We are using the same cache space. So the hit ratio is going to be the same. What we get advantage of is the overall cache latency/utility (l1*h1 + l2*h2 + l3*(1-h1+h2))
%     We use a multi tier cache that spans several of those devices
%    Our Contributions:
%         Partition the cache to different VMs according to their workload and prority
%         Calculate HRCs in realtime efficiently using a variation of Mattson's algorithm
%         Use a multi constraint optimization algorithm that uses this HRC to allocate resources
%
% For a single cache layer, if the cache is very large, would addressing be a problem?

\section{Introduction}
As the hardware keeps increasing in ``power" the ratio of the number of VMs per host also keeps increasing. A typical host in a datacenter now packs tens to hundreds of VMs on a single host in order to maximize the utilization of the resources in a given host. To this end, hosts are equiped with terabytes of hard disk space and petta bytes of externally attached storage systems.

As a result, we have managed increasing the storage capacity, but the I/O latency and throughput of these devices haven't increased at the same rate. This is due to the hardware limitation of the magnetic devices that are being used as storage devices. To overcome this limitation, datacenters speed up the I/O accesses of the VMs by using flash devices such as SSD's, Pcie SSD's, etc. as a caching device on the hosts.

While the use of the host side flash caching can improve the speedup significantly, it can also hinder the access if the cache is not managed properly. The problem with these systems are that they are throughput oriented and does not take into consideration the access pattern of the VMs. As a result a ``bad" VM with huge number of random I/O that cannot benefit from the cache might end up taking all the cache space, despite there being other interactive VMs that can benefit the most from the cache end up with very less or in some extreme cases no cache space at all. Also, different VM's can have different priorities, and our system should make sure that the workload of the VMs should not influence the priorities or Service Level Agreements (SLA) of the VMs.

A perfect caching technique should avoid contention of I/O amongst the VMs, and should be able to guarentee fine tuned I/O performance for each of the VMs on any given host.

In this work we present a simulated multi-tier flash cache based solution that uses multiple flash caches and partitions them dynamically in runtime based on the VMs' workload and priority.

Our contributions are:
\begin{itemize}
\item Present a cache partitioning algorithm that partitions two-level caches simultaneously at runtime based on individual VMs' workload and priority.
\item Calculate Hit Ratio Curves in runtime using a variation of Mattson's algorithm. Our variation runs faster and uses very low memory. (We need to quantify all these), and use a multi constraint optimization algorithm that uses the HRC to allocate resources among different caching devices.
\item Simulate two different cache devices of varying throughput and latency. Using these caches with our algorithm we show that we could attain the maximum hit rate in most cases.\note{is it hit rate/cache utility/overall speedup/performance?}
\end{itemize}
