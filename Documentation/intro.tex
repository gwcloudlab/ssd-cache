% What is the problem with the current work?
%     HDD accesses are slow
%     Number of VMs in a host is every increasing
%         More contention
%         Each VM has different workloads and different priorities

% What is our vision for the perfect world?
%     Little or no contention between VMs
%     Fine tune SLAs/ priorities -> maybe
%     As cheaply and efficiently as possible
%
% How does our system help with this vision?
%     We have different storage devices/systems that we can use such as SSD, PCIe SSD, NVM, etc.
%         Each storage has different cost and tradeoffs
%         (What do we want to use and why?)
%             - We are using the same cache space. So the hit ratio is going to be the same. What we get advantage of is the overall cache latency/utility (l1*h1 + l2*h2 + l3*(1-h1+h2))
%     We use a multi tier cache that spans several of those devices
%    Our Contributions:
%         Partition the cache to different VMs according to their workload and prority
%         Calculate HRCs in realtime efficiently using a variation of Mattson's algorithm
%         Use a multi constraint optimization algorithm that uses this HRC to allocate resources
%
% For a single cache layer, if the cache is very large, would addressing be a problem?

\section{Introduction}
As hardware increases in power, the number of VMs per host also keeps increasing. A typical host in a datacenter now packs tens to hundreds of VMs on a single host in order to maximize resource utilization---the popular VMware ESX hypervisor has increased the number of VMs supported per machine from XXXX to YYYY in recent years. To support this, hosts are now equipped with Terabytes of hard disk space and Pettabytes of externally attached storage systems.

While low cost hard drives have allowed data centers to meet this growing capacity, the I/O latency and throughput of these devices have stagnated. This is due to the hardware limitations of magnetic hard disk drives with spinning platters. Moreover, the fraction of random I/O operations on these devices tends to increase as more VMs are consolidated to a host. To overcome this limitation, studies have been done to analyse the feasibility of replacing the hard disk devices entirely with solid state drives (SSDs)~\cite{narayanan_migrating_2009}. Instead, a more practical approach that modern datacenters take to speed up the I/O accesses of the VMs is by using faster drives such as SSDs as caching devices on the hosts.

In the coming years, a deeper hierarchy of storage devices is expected to emerge, each with differing latency, throughput, price, and capacity characteristics~\cite{storage-hierarchy}. This offers new opportunities for efficiently managing data center storage, but it also complicates the picture by providing a diverse set of options to choose from. Building an efficient caching system is further compounded by the varied workload needs of different VMs. As a result a poor locality VM with a huge number of random I/Os that cannot benefit from a cache might end up greedily taking all the cache space, despite there being other VMs that could benefit more highly. Also, different VMs can have different priorities, so a cache management system should ensure that interference does not impact the Service Level Agreements (SLA) of the VMs.

% A perfect caching technique should avoid contention of I/O amongst the VMs, and should be able to guarantee fine tuned I/O performance for each of the VMs on any given host.

In this work we present a multi-tier cache management solution that dynamically partitions a set of storage devices at runtime based on the VMs' workload and priority. Our contributions are:
\begin{itemize}
\item Workload characterization and cache utility models that predict how different VMs will benefit from each tier in the cache.
\item A cache partitioning algorithm that partitions the cache layers at runtime to maximize overall performance and account for priority levels.
\item A simulation platform to evaluate our cache partitioning algorithm on cache devices of varying throughput and latency.
\end{itemize}
